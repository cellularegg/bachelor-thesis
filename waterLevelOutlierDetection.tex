\chapter{Outlier Detection based on Water Level Data}
FloodAlert (or ``Pegelalarm'' in German) is a service that provides water levels from about 30,000 measurement stations. It provides notifications to registered individuals, when a certain water level threshold is reached. Thus warning people about possible floods in their area. \cite{strassmayrFloodAlertWaterLevels}


Unfortunately sometimes an incorrect water level is reported by the sensors, which are not maintained by the team of FloodAlert. FloodAlert just fetches the data from different maintainers of sensors. To reduce the frequency of false alarms, outliers should be identified and flagged. The theoretical parts of the previous sections are applied and tested on real-word data.
% Create a connection between the theoretical descriptions of outlier detections to a real world use case. Data from \url{https://pegelalarm.at/}
% \todo{write}
\section{What is the Goal?}
The ideal goal of FloodAlert would be to have a general model, that works on a variety of rivers. The model should be able to predict the probability of being an outlier for each water level value, without knowing future values. Thereby making it possible to classify incoming datapoints in real time, without needing future datapoints to make a prediction. Furthermore it would be ideal if the outlier detection method is able to adjust its parameters automatically for each river / water level measurement location. Because different rivers have different fluctuations in water level. Therefore each water level measurement station needs individual parameters. For example one river regularly has an increase and decrease of 10 centimeters whereas for another river 10 cm of water level variance from one datapoint to the next is definitely an outlier.
\newline
\newline
Another approach would be to also take future values into consideration when predicting the probability of a value being an outlier. This method is probably easier and likely leads to better results. However a method which does not take future values into consideration would be more useful for FloodAlert, since the values could be classified immediately.

% Describe the goal to archive: \newline
% \todo{change this}
% \begin{itshape}
%     ``
%     \begin{enumerate}
%         \item We are looking for an algorithm that detects outliers using only historical values. This would allow us to assign a probability to the last measured water level of a station, which would indicate how likely it is to be an outlier. We would then not store outliers in our system at all or classify them as outliers from the beginning.
%         \item For us also an algorithm would be helpful, which assigns an outlier probability to each arbitrary measured value X of a time series. This algorithm would not only have access to the measured values before it, but also to those after it. This would allow us to detect outliers for all the time series data that we already have in the system and, for example, delete them.

%     \end{enumerate}
%     Point 2 is probably easier to implement than point 1, so an algorithm 1 would be more helpful for us.
%     Also important would be that the algorithm adjusts its (hyper)parameters accordingly based on the historical data. This means that a level at which there are often strong fluctuations, an outlier must already be quite outlier so that it is considered as an outlier.
%     ''
% \end{itshape}

\section{How to retrieve the Data (Description of the API)}
% Short overview on how to use the API to retrieve data? Python project to retrieve data: \url{https://github.com/SOBOS-GmbH/pegelalarm_public_pas_doc}
To make the access to the API easier SOBOS GmbH developed a Python wrapper which returns the requested data as a Pandas dataframe.\cite{PandasDocumentationPandas} The code for the Python wrapper is available on GitHub\cite{GitHub}: \url{https://github.com/SOBOS-GmbH/pegelalarm_public_pas_doc} \cite{strassmayrPegelalarmAPIWrapper2022}

In order to request data, an API key needs to be requested using credentials. To request the API key a POST request needs to be sent to this endpoint \url{https://api.pegelalarm.at/api/login}, where the request body contains the users' credentials as shown in \autoref{listing:api-key-request-body}. 
\begin{listing}
\begin{minted}[linenos]{json}
{
    "username": "myUsername",
    "password": "myPassword"
}
\end{minted}
\caption{Request body to get API key}
\label{listing:api-key-request-body}
\end{listing}
To generate the actual API key the key from the response needs to be hashed using the \ac{HMAC} Algorithm. This is done using Python's built in ``hmac'' module. \cite{HmacKeyedHashingMessage}. Afterwards the \ac{HMAC} is byte64 encoded, so it can be sent in the ``X-AUTH-TOKEN'' header field.

To get the unique identifier for a specific measurement station the list endpoint can be used. Which accepts three optional query parameters (qStationName - station name, qWater - water name and commonid - the unique identifier of a station) and returns a list of matching stations with metadata like coordinates, country or last water level. E.g. to retrieve the identifier of the Danube station located in Linz the URL is the following: \url{https://api.pegelalarm.at/api/station/1.1/list?qStationName=Linz&qWater=Donau}. Keep in mind in order to retrieve any data the header value ``X-AUTH-TOKEN'' must be set. An example response of this request is shown in \autoref{listing:response-list-endpoint}. Where the unique identifier is the ``commonid'' in line 9.
\begin{listing}
\begin{minted}[linenos]{json}
{
    "status": {
        "code": 200
    },
    "payload": {
        "stations": [
            {
                "name": "Donau / Linz / at",
                "commonid": "207068-at",
                "country": "Österreich",
                "stationName": "Linz",
                "water": "Donau",
                "region": "Oberösterreich",
                "latitude": 48.306915712282,
                "longitude": 14.284689597541,
                "positionKm": 2135.17,
                "altitudeM": 247.74,
                "defaultWarnValueCm": 550.0,
                "defaultAlarmValueCm": 630.0,
                "data": [
                    {
                        "type": "height in cm",
                        "value": 358.0,
                        "requestDate": "19.04.2022T14:59:51+0200",
                        "sourceDate": "19.04.2022T14:45:00+0200"
                    }
                ],
                "trend": 10,
                "situation": 10,
                "visibility": "PUBLIC",
                "stationType": "surfacewater"
            }
        ]
    }
}
\end{minted}
\caption{Example response of the list endpoint}
\label{listing:response-list-endpoint}
\end{listing}

To retrieve historical data, the history endpoint can be used. The request URL has the following structure: \url{https://api.pegelalarm.at/api/station/1.1/<unit>/<commonid>/history?<parameters>}. The unit can either be ``height'' or ``flow''. For this thesis only height data was used. The following parameters can be set:
\begin{itemize}
    \item \textbf{loadStartDate}: The start timestamp of the queried data.
    \item \textbf{loadEndDate}: The end timestamp of the queried data.
    \item \textbf{granularity}: The granularity of the response. 
\end{itemize}
Possible values for the granularity are: ``raw'' (for the last 3 months of data), ``hour'', ``day'', ``month'', ``year'' or ``era'' (era returns one value for a given time)
When requesting aggregated data (anything other than ``raw''), the maximum value of the timespan is used. The timestamps are in the following format: ``<DD>.<MM>.<YYYY>T<HH>:<MM>:<SS><+-UTCOFFSET>'', where the `+' is URL encoded to ``\%2B''. E.g.: ``31.03.2022T13:35:40\%2B0200''. If no parameters are provided the API returns the last few datapoints.

An example request would be: \url{https://api.pegelalarm.at/api/station/1.1/height/207068-at/history?loadStartDate=01.03.2022T13:35:40%2B0200&loadEndDate=01.03.2022T18:00:00%2B0200&granularity=hour} The result of this request is shown in \autoref{listing:water-level-data-response}.

\begin{listing}
\begin{minted}[linenos]{json}
{
    "status": {
        "code": 200
    },
    "payload": {
        "history": [
            {
                "value": 360.0,
                "sourceDate": "01.03.2022T13:00:00+0100"
            },
            {
                "value": 360.0,
                "sourceDate": "01.03.2022T14:00:00+0100"
            },
            {
                "value": 359.0,
                "sourceDate": "01.03.2022T15:00:00+0100"
            },
            {
                "value": 361.0,
                "sourceDate": "01.03.2022T16:00:00+0100"
            },
            {
                "value": 362.0,
                "sourceDate": "01.03.2022T17:00:00+0100"
            }
        ]
    }
}
\end{minted}
\caption{Example response of historical water level data for one station}
\label{listing:water-level-data-response}
\end{listing}

\section{Overview of the Data}
To test and tune the outlier detection methods the entire history of data was used. The granularity of the data usually is hourly up to the last three months of data. There are a few exceptions, where the granularity is not hourly, because either one or a few datapoints are missing. For the last three months the granularity is ``raw'', which is different for each station. Additionally the base (where the sensor reports water level equals zero) also differs per station. Some station have an arbitrary zero level whereas some stations have the sea level as a base. Thus the water level does not actually reflect the actual height, or depth, of the water. Instead it represents the relative water level to a specific height.
\par
The following water level measurement stations were used to test the outlier detection methods.
\change{Check and maybe remove automatic hyphenation}
% \begin{itemize}
%     % \item List of station addresses \todo{add final stations} 
%     \todo{add location description to station list}
%     \item Station Aghacashlaun, Aghacashlaun (36022-ie)
%     \item Station 2386-ch
%     \item Station 2720050000-de
%     \item Station 42960105-de
%     \item Station 39003-ie
% \end{itemize}
\subsubsection{Station Aghacashlaun, Aghacashlaun (36022-ie)}
This station, on the river Aghacashlaun and in the area of Aghacashlaun, is located in northern Ireland. The coordinates of this station are: 54.03647, -7.94672 (\ac{Lat}, \ac{Long}). \cite{StationAghacashlaunAghacashlaun}

\subsubsection{Station Murg, Frauenfeld (2386-ch)}
The station, on the river Murg and in the area of Frauenfeld, is located in northern Switzerland. The coordinates of this station are: 47.56852, 8.89432 (\ac{Lat}, \ac{Long}). \cite{StationMurgFrauenfeld}

\subsubsection{Station Sieg, Betzdorf (2720050000-de)}
The station, on the river Sieg and in the area of Betzdorf, is located in western Germany. The coordinates of this station are: 50.79332, 7.86390 (\ac{Lat}, \ac{Long}). \cite{StationSiegBetzdorf}

\subsubsection{Station Losse, Helsa (42960105-de)}
The station, on the river Losse and in the area of Helsa, is located in central Germany . The coordinates of this station are: 51.25574, 9.68537 (\ac{Lat}, \ac{Long}). \cite{StationLosseHelsa}

\subsubsection{Station Crana, Tullyarvan (39003-ie)}
The station, on the river Crana and in the area of Tullyarvan, is located in northern Ireland. The coordinates of this station are: 55.14356, -7.45239 (\ac{Lat}, \ac{Long}). \cite{StationCranaTullyarvan}

\section{Manually detect outliers for a subset of data}
% \todo{add examples of outliers in the data}
% \todo{Describe how the outliers are stored?}
% Show cases of outliers in the data and manually classify them. (Also define a way/data structure to classify outliers for time series data) \todo{Remove this}
In order to speed up the manual labeling of outliers a program was written. The program is a Plotly Dash \cite{DashDocumentationUser} \change{Which reference to add? Website, Documentation or GitHub?} web application which displays the water level data as a scatter chart. 
By clicking the datapoints in the chart the user is able to toggle the datapoint as an outlier or back to a regular value. 
In Listing \ref{listing:manual-outlier-selection} the source code of the Dash application is shown. 
In \autoref{figure:manual-outlier-selection} the Website of the Python app is shown, the red dots between 25 and October 28 are already classified outliers. Below the chart a rangeslider is located, to move the zoomed in view horizontally. 
The refresh button on the left side refreshes the graph, thus updating the color and label of previously selected outliers. Furthermore it saves the data to a parquet file.
The upper and lower limit of the y-axis also gets updated, when pressing the refresh button. 
The limits are automatically set to the lowest and highest regular value. 
This was implemented, because some datasets had outliers with a huge difference towards the regular data. 
Without the automatic scaling of the y-axis, detecting other outliers with a smaller difference was not possible.
% \begin{center}
%     \makebox[\textwidth]{\includegraphics[width=\paperwidth]{...}}
%   \end{center}
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{./pics/manual-outlier-selection.png}
    \caption{Dash Webapp to classify outliers}
    \label{figure:manual-outlier-selection}
\end{figure}

% https://tex.stackexchange.com/questions/368864/pagebreak-for-minted-in-figure
% \begin{listing}
% \begin{minted}[frame=single]{python}
% print('hw')
% \end{minted}
% \caption{Code example with simple formatting}
% \label{code:manual-outlier-selection}
% \end{listing}

\subsubsection{Manual Outlier Detection Webapp using Dash}
\inputminted[linenos]{python}{./code/manual_outlier_detection.py}
\captionof{listing}{Manual Outlier Detection Webapp using Dash\label{listing:manual-outlier-selection}}
\todo{add comments to code}

\section{Explorative Data Analysis}
\todo{add another station}
\todo{add line chart over whole time with 1-2 zooms}
\subsection{Aghacashlaun Station}
% Similar to overview of the data
% \begin{figure}[H]
%     \centering
%     \includegraphics{./plots/pdfs/36022-ie/boxplot_36022-ie_all.pdf}
%     \caption{Boxplot of Aghacashlaun - Aghacashlaun containing all values}
%     \label{figure:boxplot-36022-ie-all}
% \end{figure}
% \begin{figure}[H]
%     \centering
%     \includegraphics{./plots/pdfs/36022-ie/boxplot_36022-ie_regular.pdf}
%     \caption{Boxplot of Aghacashlaun - Aghacashlaun containing regular values}
%     \label{figure:boxplot-36022-ie-regular}
% \end{figure}

% \begin{figure}[H]
%     \centering
%     \includegraphics{./plots/pdfs/36022-ie/boxplot_36022-ie_outlier.pdf}
%     \caption{Boxplot of Aghacashlaun - Aghacashlaun containing outlier values}
%     \label{figure:boxplot-36022-ie-outlier}
% \end{figure}
\todo{change formatting of tables}
\input{tables/36022-ie/36022-ie-7-number-summary-all}
\input{tables/36022-ie/36022-ie-7-number-summary-regular.tex}
\input{tables/36022-ie/36022-ie-7-number-summary-outlier.tex}

\begin{figure}[htp]
    \centering
    \includegraphics{./plots/pdfs/36022-ie/outlier_class_distribution_36022-ie.pdf}
    \caption{Class distribution of Aghacashlaun - Aghacashlaun}
    \label{figure:class-distribution-36022-ie}
\end{figure}

\begin{figure}[htp]
    \centering
    \includegraphics{./plots/pdfs/36022-ie/boxplot_36022-ie.pdf}
    \caption{Boxplot of Aghacashlaun - Aghacashlaun}
    \label{figure:boxplot-36022-ie}
\end{figure}

\begin{figure}[htp]
    \centering 
    \includegraphics{./plots/pdfs/36022-ie/water_level_histogram_36022-ie.pdf}
    \caption{Histogram of the water level of Aghacashlaun - Aghacashlaun}
    \label{figure:water-level-histogram-36022-ie}
\end{figure}

\begin{figure}[htp]
    \centering
    \includegraphics{./plots/pdfs/36022-ie/water_level_delta_histogram_36022-ie.pdf}
    \caption{Histogram of the water level delta of Aghacashlaun - Aghacashlaun}
    \label{figure:water-level-delta-histogram-36022-ie}
\end{figure}

\begin{figure}[htp]
    \centering
    \includegraphics{./plots/pdfs/36022-ie/time_delta_histogram_36022-ie.pdf}
    \caption{Histogram of the time delta (in hours) of Aghacashlaun - Aghacashlaun}
    \label{figure:time-delta-histogram-36022-ie}
\end{figure}
\par
Code: \cite{zelenayOutlierDetectionWater2022}
\todo{add descriptions to tables and figures}

\section{Outlier Detection performance Metrics}
\subsection{Confusion Matrix}
To compare the performance of two classification methods a confusion matrix, shown in \autoref{table:confusion-matrix}, can be used to provide an overview. A confusion matrix consists of the following elements (explained on the basis of outlier detection):
\begin{itemize}
    \item \ac{TP}: actual class: outlier, predicted class: outlier
    \item \ac{FN}: actual class: outlier, predicted class: regular
    \item \ac{TN}: actual class: regular, predicted class: regular
    \item \ac{FP}: actual class: regular, predicted class: outlier
\end{itemize}
\begin{table}[ht]
    \begin{tabular}{llll}
     &  & \multicolumn{2}{l}{Predicted label} \\ \cline{3-4} 
     & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{\ac{P}} & \multicolumn{1}{l|}{\ac{N}} \\ \cline{2-4} 
    \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{Positive} & \multicolumn{1}{l|}{\cellcolor[HTML]{9AFF99}True positive} & \multicolumn{1}{l|}{\cellcolor[HTML]{FFCCC9}False negative} \\ \cline{2-4} 
    \multicolumn{1}{l|}{\multirow{-2}{*}{True/ actual label}} & \multicolumn{1}{l|}{Negative} & \multicolumn{1}{l|}{\cellcolor[HTML]{FFCCC9}False positive} & \multicolumn{1}{l|}{\cellcolor[HTML]{9AFF99}True negative} \\ \cline{2-4} 
\end{tabular}
\caption{Example of binary confusion matrix}
\label{table:confusion-matrix}
\end{table}
% ausser publikation welche nur cm behandelt
% \todo{add reference? \url{https://en.wikipedia.org/w/index.php?title=Confusion_matrix&oldid=1058352752}}
The green cells represent the correctly classified values (\ac{TP} \& \ac{TN}) and the red cells represent the incorrectly classified values (\ac{FN} \& \ac{FP}). The confusion matrix is the basis of many more performance metrics.
\subsection{Accuracy}
To make the comparison easier, the confusion matrix can be aggregated into one single value as a metric. There are numerous ways how this single metric can be calculated. One of the Most common and basic methods is the Accuracy:
\begin{equation}
    Accuracy = \frac{Number\,of\,correct\,predictions}{Total\,number\,of\,predictions} = \frac{TP + TN}{TP + FN + TP + FP} = \frac{TP + TN}{P + N}
\end{equation}
The accuracy provides information what percentage of values are correctly classified. However if the classes are not equally distributed, the accuracy is a bad metric. This is especially true for heavily imbalanced classes. For example if the dataset has $1\,000\,000$ positive values and $10$ negative, when optimizing for a high accuracy it can happen that the classifier predicts every value as positive and still achieves a very high accuracy:
\begin{equation}
    Accuracy = \frac{1\,000\,000 + 0}{1\,000\,000 + 10} = 0.99999
\end{equation}
Thus for measuring the performance of outliers, the accuracy is not a suitable metric.

\subsection{Precision and Recall}
\autoref{figure:precision-vs-recall} shows a visual comparison between precision and recall. The precision provides information about what percentage of positive predictions were actually correct. Whereas the recall provides information about what percentage of all positive values were actually predicted as such. Precision and Recall are used to calculate the F-Score described in \autoref{subsection:f-score}.
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth/2]{./pics/Precisionrecall.pdf}
    \caption{Precision vs Recall\cite{PrecisionRecall2022}}
    \label{figure:precision-vs-recall}
\end{figure}
% \todo{Correct reference of a image used 1:1? -> yes}
\subsubsection{Precision}
\begin{equation}
    Precision = \frac{TP}{TP + FP}
\end{equation}

\subsubsection{Recall}
\begin{equation}
    Recall = \frac{TP}{TP + FN}
\end{equation}

\subsection{F-score}
\label{subsection:f-score}
The $F_1-score$ is the harmonic mean of the precision and recall.
\begin{equation}
    F_1-score = \frac{2}{\frac{1}{Precision} + \frac{1}{Recall}} = \frac{2}{\frac{Precision + Recall}{Precision \cdot Recall}} = \frac{2 \cdot Precision \cdot Recall}{Precision + Recall}
\end{equation}
\cite{sasakiTruthFmeasure, chinchorMUC4EvaluationMetrics1992}

The $F_\beta-score$ is more sophisticated compared to the $F_1-score$ where an additional parameter ($\beta$) needs to be chosen, which is used to weight the precision.
\begin{equation}
    F_\beta-score = \frac{(\beta^2 + 1) \cdot Precision \cdot Recall}{\beta^2 \cdot Precsion + Recall}
\end{equation}
\cite{tahaMetricsEvaluating3D2015, chinchorMUC4EvaluationMetrics1992}
% This \cite{tahaMetricsEvaluating3D2015} references \cite{chinchorMUC4EvaluationMetrics1992}.\todo{chech whether this can also be included} often both are used.


The $F_1-score$ uses $1$ for $\beta$ so the precision and the recall are equally weighted.

% Why accuracy is bad in our case. Precision, Recall, F1 score

% Define a way to compare different outlier detection models / define performance metrics. E.g. number correct outliers, average confidence for the correct outliers, number of missed outliers,....

\section{Implementation of different Outlier Detection Approaches}

The input data structure for the different approaches is a pandas DataFrame.\cite{PandasDocumentationPandas} The structure of the DataFrame is equal for all measurement stations. It contains the following columns:
\begin{itemize}
    \item \verb|water_level|: the water level of the river in cm (datatype: float64)
    \item \verb|timestamp|: the date and time of the datapoint in UTC (datatype: datetime64[ns, UTC])
    \item \verb|is_outlier|: true or false depending if the value is an outlier (datatype: bool)
\end{itemize}
To test the performance of the models the column \verb|is_outlier| contains the ground truth, which was manually labeled. An example of the input data can be seen in \autoref{table:36022-ie-9head}.

\input{tables/36022-ie/36022-ie-9head.tex}
To speed up the process of calculating the result of the outlier detection method ($|x_t - \hat{x}_t|$, the z-score or the modified z-score) is all done in one step and in the second step the result of this calculation is compared against a range of thresholds.
\subsection{Mean Threshold}
Calculating $\hat{x}_t$ using the mean for a pandas DataFrame \cite{PandasDocumentationPandas} is quite straightforward and is shown in \autoref{listing:mean-outlier-calculation}. To reduce the amount of duplicate code, the imports are only included in \autoref{listing:mean-outlier-calculation}.
\begin{listing}
\begin{minted}[linenos]{python}
from typing import Union

import numpy as np
import pandas as pd


def mean_outlier_detection(input_df: pd.DataFrame,
                           window: Union[int, None],
                           center_window: bool):
    """
    Detects outliers in a dataframe using a (moving) average.
    :param input_df: the input dataframe where the values are stored
                     in the column water_level
    :param window: the size of the window, None if no window should
                   be used
    :param center_window: whether the window should be centered or not
    :return: a copy of the input dataframe where the column result
             should be compared to a threshold to detect outliers
    """
    od_df = input_df.copy()
    if window is None:
        od_df['x_hat'] = od_df['water_level'].mean()
    else:
        od_df['x_hat'] = \
            od_df['water_level'].rolling(window=window,
                                         center=center_window,
                                         min_periods=1).mean()
    od_df['result'] = np.abs(od_df['water_level'] - od_df['x_hat'])
    return od_df
\end{minted}
\caption{First step of classifying outliers using the mean}
\label{listing:mean-outlier-calculation}
\end{listing}

\subsection{Median Threshold}
The calculation of the median is quite similar to the mean. It is shown in \autoref{listing:median-outlier-calculation}.
\begin{listing}
\begin{minted}[linenos]{python}
def median_outlier_detection(input_df: pd.DataFrame,
                             window: Union[int, None],
                             center_window: bool):
    """
    Detects outliers in a dataframe using a (moving) average.
    :param input_df: the input dataframe where the values are stored
                     in the column water_level
    :param window: the size of the window, None if no window should
                   be used
    :param center_window: whether the window should be centered or not
    :return: a copy of the input dataframe where the column result
             should be compared to a threshold to detect outliers
    """
    od_df = input_df.copy()
    if window is None:
        od_df['x_hat'] = od_df['water_level'].median()
    else:
        od_df['x_hat'] = \
            od_df['water_level'].rolling(window=window,
                                         center=center_window,
                                         min_periods=1).median()
    od_df['result'] = np.abs(od_df['water_level'] - od_df['x_hat'])
    return od_df
\end{minted}
\caption{First step of classifying outliers using the median}
\label{listing:median-outlier-calculation}
\end{listing}

% \subsection{\ac{MAD} Threshold}
\subsection{MAD Threshold}
In \autoref{listing:mad-outlier-calculation} an example for an implementation for calculating the \ac{MAD} is provided.
\begin{listing}
\begin{minted}[linenos]{python}
def mad_outlier_detection(input_df: pd.DataFrame,
                          window: Union[int, None],
                          center_window: bool):
    """
    Detects outliers in a dataframe using a (moving) average.
    :param input_df: the input dataframe where the values are stored
                     in the column water_level
    :param window: the size of the window, None if no window should
                   be used
    :param center_window: whether the window should be centered or not
    :return: a copy of the input dataframe where the column result
             should be compared to a threshold to detect outliers
    """
    od_df = input_df.copy()
    if window is None:
        od_df['x_hat'] = np.median(
            np.abs(od_df['water_level'] - np.median(
                od_df['water_level'])))
    else:
        od_df['x_hat'] = \
            od_df['water_level'].rolling(window=window,
                                         center=center_window,
                                         min_periods=1).apply(
                lambda x: np.median(np.abs(x - np.median(x))))
    od_df['result'] = np.abs(od_df['water_level'] - od_df['x_hat'])
    return od_df
\end{minted}
\caption{First step of classifying outliers using the \ac{MAD}}
\label{listing:mad-outlier-calculation}
\end{listing}

\subsection{Z-Score}
\autoref{listing:z-score-outlier-calculation} shows how to calculate th z-score in Python.
\begin{listing}
\begin{minted}[linenos]{python}
def z_score_outlier_detection(input_df: pd.DataFrame,
                              window: Union[int, None],
                              center_window: bool):
    """
    Detects outliers in a dataframe using a (moving) average.
    :param input_df: the input dataframe where the values are stored
                     in the column water_level
    :param window: the size of the window, None if no window should
                   be used
    :param center_window: whether the window should be centered or not
    :return: a copy of the input dataframe where the column result
             should be compared to a threshold to detect outliers
    """
    od_df = input_df.copy()
    if window is None:
        od_df['mean'] = od_df['water_level'].mean()
        od_df['std'] = od_df['water_level'].std()
    else:
        od_df['mean'] = \
            od_df['water_level'].rolling(window=window,
                                         center=center_window,
                                         min_periods=1).mean()
        od_df['std'] = \
            od_df['water_level'].rolling(window=window,
                                         center=center_window,
                                         min_periods=1).std()
    od_df['result'] = \
        (od_df['water_level'] - od_df['mean']).divide(od_df['std'])
    return od_df
\end{minted}
\caption{First step of classifying outliers using the z-score}
\label{listing:z-score-outlier-calculation}
\end{listing}

\subsection{Modified z-score}
The code for calculating the result of the modified z-score (using the \ac{MADN}) is shown in \autoref{listing:madn-z-score-outlier-calculation}.
\begin{listing}
\begin{minted}[linenos]{python}
def madn_z_score_outlier_detection(input_df: pd.DataFrame,
                                   window: Union[int, None],
                                   center_window: bool):
    """
    Detects outliers in a dataframe using a (moving) average.
    :param input_df: the input dataframe where the values are stored
                     in the column water_level
    :param window: the size of the window, None if no window should
                   be used
    :param center_window: whether the window should be centered or not
    :return: a copy of the input dataframe where the column result
             should be compared to a threshold to detect outliers
    """
    od_df = input_df.copy()
    if window is None:
        od_df['median'] = od_df['water_level'].median()
        od_df['mad'] = np.median(
            np.abs(od_df['water_level'] - od_df['median']))
        od_df['madn'] = od_df['mad'] / 0.6745
    else:
        od_df['median'] = \
            od_df['water_level'].rolling(window=window,
                                         min_periods=1,
                                         center=center_window).median()
        od_df['mad'] = \
            od_df['water_level'].rolling(window=window,
                                         min_periods=1,
                                         center=center_window).apply(
                lambda x: np.median(np.abs(x - np.median(x))))
        od_df['madn'] = od_df['mad'] / 0.6745
    od_df['result'] = \
        (od_df['water_level'] - od_df['median']).abs() \
            .divide(od_df['madn'])
    return od_df
\end{minted}
\caption{First step of classifying outliers using the modified z-score (\ac{MADN}-z-score)}
\label{listing:madn-z-score-outlier-calculation}
\end{listing}

\subsection{Preprocessing the Data}\label{subsection:data-preprocessing}
In the preprocessing step outliers which extremely vary from the usual trend of the other values were removed. This was done by defining and upper and lower limit for the data. If a value is not inside this limit it is removed from the dataset. The thought behind preprocessing the data was to increase the model performance by removing extreme outlier values from the beginning. The disadvantage of the preprocessing step is, that two limits need to be defined for each measurement station. The limits need to be chosen carefully. On the one hand, if the valid value range is too large, no extreme outliers are removed and the preprocessing is useless, on the other hand if the valid value range is too small the preprocessing might remove valid values, which would indicate a possible flood. Thus it is also a good idea to regularly check and maybe update those limits.
\par
Core Url: \url{https://github.com/cellularegg/bachelor-thesis-code/blob/main/preprocessing.ipynb}\todo{add code to appendix}

\subsection{Finding Parameters}
To find the best parameters a grid search was used. The first step was to calculate the results for different methods, window sizes and types (centered an non centered). For each unique parameter combination the result dataframe was stored as a file, where the filename provided information about the parameters. An example for a filename would be ``\verb|12_cw_median.parquet|'', where ``\verb|12|'' is the window size, ``\verb|cw|'' stands for center window (``\verb|nocw|'' is for no center window) and ``\verb|median|'' is the method used. This was done for every unique combination of parameters. The possible values for each parameter, that was used, is listed below.
\begin{itemize}
    \item \textbf{normalized}: yes, no
    \item \textbf{preprocessed}: yes, no 
    \item \textbf{window size}: None, 2-51 (in steps of one)
    \item \textbf{centered window}: yes, no
    \item \textbf{method}: mean, median, mad, z-score, modified z-score (madn-z-score)
    \item \textbf{common-id}: ``36022-ie'', ``39003-ie'', ``2386-ch'', ``42960105-de'', ``2720050000-de''
\end{itemize}
Due to the large number of unique combinations of parameters the size of the resulting files was quite large (multiple Gigabytes). The grid search was executed in parallel using multiprocessing, in order to speed up the process.
\par
After the results of each parameter was calculated, for each file a range of thresholds was tested and the performance (confusion matrix and $F_1-score$) of this threshold and the parameters used were saved to a DataFrame. This was also conducted in parallel to decrease the runtime.
% Develop different outlier detection methods in Python and calculate performance metrics for each
\par
Url of Code: \url{https://github.com/cellularegg/bachelor-thesis-code/blob/main/threshold_based_outlier_detection.py} \todo{include code in appendix}

\section{Compare different Outlier Detection Approaches}
As expected, using a centered window yields the best $F_1-score$. Furthermore the methods with the best performance are the median and the \ac{MADN} z-score. While the median performed best with smaller window sizes (5-7) the modified z-score performed best with larger window sizes (22-28). Using the \ac{MAD} to calculate the $\hat{x_t}$ yields the worst performance.
\par
\autoref{table:top-avg-predictions-summary} shows the top 5 average $F_1-scores$ of all stations tested. When using one set of parameters for different measurement stations the median performed best. Using the mean for $\hat{x_t}$ delivered the second best performance, when comparing shared parameters bewteen all tested stations. However the average $F_1-score$ of the mean is only 0.47794, it was reached by using a centered window of size 5, a threshold of 18.87960 and not normalizing the data. Due to the heterogeneity of the fluctuations of the water levels for the different stations it is not recommended to use the same parameters for different stations. Similar or equal parameters should only be used when the water levels of two measurement stations behave similarly. Additionally when using the same parameters for different stations the performance of those parameters should be looked at per station and not as an average of the  $F_1-scores$. This hinders the fact that one model performs perfectly ($F_1-score = 1$) and the other very poorly ($F_1-score = 0.5$). \todo{change average to harmonic mean?}
\input{tables/top-avg-predictions-summary.tex}
% \todo{compare parameters for different stations and check whether there is a parameter that can universally be used}
\par
Using the preprocessing (described in \autoref{subsection:data-preprocessing}) usually resulted in a lower performance, when comparing the hightest $F_1-scores$ for both datasets (preprocessed and not preprocessed). The reason for this is, that the outliers removed by the two sided filter were detected as outliers anyway. So just the number of \acp{TP} was reduced. Thus resulting in a slightly lower overall performance, since the outliers were completely removed from the dataset in the preprocessing step.

\todo{add specific examples for FP and FN}
\subsubsection{Mean}
\autoref{figure:od-mean-39003-ie} shows the best performing outlier detection using mean. The $F_1-score$ is about 0.68. % 0.6842105263157895
\begin{figure}[htp]
    \centering 
    \includegraphics{plots/pdfs/39003-ie/od_mean_39003-ie_all.pdf}
    \caption{Best performance of outlier detection using mean (Crana - Tullyarvan)}
    \label{figure:od-mean-39003-ie}
\end{figure}

\subsubsection{Median}
\autoref{figure:od-median-2720050000-de} shows the best performing outlier detection using mean. The $F_1-score$ is about 0.91. % 0.9051094890510949
\begin{figure}[htp]
    \centering 
    \includegraphics{plots/pdfs/2720050000-de/od_median_2720050000-de_all.pdf}
    \caption{Best performance of outlier detection using median (Sieg - Betzdorf)}
    \label{figure:od-median-2720050000-de}
\end{figure}
\subsubsection{MAD}
\autoref{figure:od-mad-42960105-de} shows the best performing outlier detection using mean. The $F_1-score$ is about 0.45. % 0.45454545454545453
\begin{figure}[htp]
    \centering 
    \includegraphics{plots/pdfs/42960105-de/od_mad_42960105-de_all.pdf}
    \caption{Best performance of outlier detection using mad (Losse - Helsa)}
    \label{figure:od-mad-42960105-de}
\end{figure}

\subsubsection{Z-score}
\autoref{figure:od-z-score-39003-ie} shows the best performing outlier detection using mean. The $F_1-score$ is about 0.68. % 0.6776677667766777
\begin{figure}[htp]
    \centering 
    \includegraphics{plots/pdfs/39003-ie/od_z-score_39003-ie_all.pdf}
    \caption{Best performance of outlier detection using mad (Crana - Tullyarvan)}
    \label{figure:od-z-score-39003-ie}
\end{figure}

\subsubsection{MADN-z-score}
\autoref{figure:od-madn-z-score-36022-ie} shows the best performing outlier detection using mean. The $F_1-score$ is about 0.79. % 0.7892049598832969
\begin{figure}[htp]
    \centering 
    \includegraphics{plots/pdfs/36022-ie/od_mad-z-score_36022-ie_all.pdf}
    \caption{Best performance of outlier detection using \ac{MADN}-z-score (Aghacashlaun - Aghacashlaun)}
    \label{figure:od-madn-z-score-36022-ie}
\end{figure}

\subsection{Station 36022-ie}
\autoref{table:36022-ie-top-predictions-summary} shows the top three predictions per model for normalized and not normalized data. It clearly pictures, that methods using a centered moving window perform better. It is also interesting to see that the \ac{MADN} z-score and the regular z-score performed equally well regardless of the fact that the data is normalized or not. \todo{think of explanation for that.}
\todo{change mad-z-score to madn-z-score}
\input{tables/36022-ie/36022-ie-top-predictions-summary.tex}

\subsection{Station 2386-ch}
In \autoref{table:2386-ch-top-predictions-summary} the top three predictions per model for normalized and not not normalized data is shown. The outlier detection for this station has the worst performance among those tested. The result of this model is shown in \autoref{figure:od-result-example-2386-ch}. Every value above the threshold is classified as an outlier. Every value below the threshold is classified as a regular value.

\begin{figure}[htp]
    \centering 
    \includegraphics{plots/pdfs/2386-ch/od_result_median_2386-ch_all.pdf}
    \caption{Result of median based outlier detection Murg - Frauenfeld}
    \label{figure:od-result-example-2386-ch}
\end{figure}


\input{tables/2386-ch/2386-ch-top-predictions-summary.tex}

\subsection{Station 2720050000-de}
In \autoref{table:2720050000-de-top-predictions-summary} the top three predictions per model for normalized and not not normalized data is shown. With an $F_1-score$ of 0.905109 this station had the best performance. The result of this model is shown in \autoref{figure:od-result-example-202720050000-de}. Every value above the threshold is classified as an outlier. Between 2018 and 2019 an example of a \ac{FP} can be seen (the orange values above the threshold).

\begin{figure}[htp]
    \centering 
    \includegraphics{plots/pdfs/2720050000-de/od_result_median_2720050000-de_all.pdf}
    \caption{Result of median based outlier detection Sieg - Betzdorf}
    \label{figure:od-result-example-202720050000-de}
\end{figure}

\input{tables/2720050000-de/2720050000-de-top-predictions-summary.tex}

\subsection{Station 42960105-de}
In \autoref{table:42960105-de-top-predictions-summary} the top three predictions per model for normalized and not not normalized data is shown.
\input{tables/42960105-de/42960105-de-top-predictions-summary.tex}

\subsection{Station 39003-ie}
In \autoref{table:39003-ie-top-predictions-summary} the top three predictions per model for normalized and not not normalized data is shown.
\input{tables/39003-ie/39003-ie-top-predictions-summary.tex}

% Compare detection methods from the previous section.
% \todo{write}

