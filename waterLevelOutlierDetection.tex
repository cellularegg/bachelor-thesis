\chapter{Outlier Detection based on Water level Data}
Create a connection between the theoretical descriptions of outlier detections to a real world use case. Data from \url{https://pegelalarm.at/}
\section{What is the Goal?}
Describe the goal to archive: \newline
\begin{itshape}
    ``
    \begin{enumerate}
        \item We are looking for an algorithm that detects outliers using only historical values. This would allow us to assign a probability to the last measured water level of a station, which would indicate how likely it is to be an outlier. We would then not store outliers in our system at all or classify them as outliers from the beginning.
        \item For us also an algorithm would be helpful, which assigns an outlier probability to each arbitrary measured value X of a time series. This algorithm would not only have access to the measured values before it, but also to those after it. This would allow us to detect outliers for all the time series data that we already have in the system and, for example, delete them.

    \end{enumerate}
    Point 2 is probably easier to implement than point 1, so an algorithm 1 would be more helpful for us.
    Also important would be that the algorithm adjusts its (hyper)parameters accordingly based on the historical data. This means that a level at which there are often strong fluctuations, an outlier must already be quite outlier so that it is considered as an outlier.
    ''
\end{itshape}

\section{How to retrieve the Data (Description of the API)}
% Short overview on how to use the API to retrieve data? Python project to retrieve data: \url{https://github.com/SOBOS-GmbH/pegelalarm_public_pas_doc}
To make the access to the API easier SOBOS GmbH developed a Python wrapper which returns the requested data as a Pandas dataframe. The code for the Python wrapper is available on GitHub\cite{GitHub}: \url{https://github.com/SOBOS-GmbH/pegelalarm_public_pas_doc} \cite{strassmayrPegelalarmAPIWrapper2022}

In order to request data, an API key needs to be requested using credentials. To request the API key a POST request needs to be sent to this endpoint \url{https://api.pegelalarm.at/api/login}, where the request body contains the users' credentials as shown in \autoref{listing:api-key-request-body}. 
\begin{listing}
\begin{minted}[linenos]{json}
{
    "username": "myUsername",
    "password": "myPassword"
}
\end{minted}
\caption{Request body to get API key}
\label{listing:api-key-request-body}
\end{listing}
To generate the actual API key the key from the response needs to be hashed using the \ac{HMAC} Algorithm. This is done using Python's built in ``hmac'' module. \cite{HmacKeyedHashingMessage}. Afterwards the HMAC is byte64 encoded, so it can be sent in the ``X-AUTH-TOKEN'' header field.

To get the unique identifier for a specific measurement station the list endpoint can be used. Which accepts three optional query parameters (qStationName - station name, qWater - water name and commonid - the unique identifier of a station) and returns a list of matching stations with metadata like coordinates, country or last water level. E.g. to retrieve the identifier of the Danube station located in Linz the URL is the following: \url{https://api.pegelalarm.at/api/station/1.1/list?qStationName=Linz&qWater=Donau}. Keep in mind in order to retrieve any data the header value ``X-AUTH-TOKEN'' must be set. An example response of this request is shown in \autoref{listing:response-list-endpoint}. Where the unique identifier is the ``commonid'' in line 9.
\begin{listing}
\begin{minted}[linenos]{json}
{
    "status": {
        "code": 200
    },
    "payload": {
        "stations": [
            {
                "name": "Donau / Linz / at",
                "commonid": "207068-at",
                "country": "Österreich",
                "stationName": "Linz",
                "water": "Donau",
                "region": "Oberösterreich",
                "latitude": 48.306915712282,
                "longitude": 14.284689597541,
                "positionKm": 2135.17,
                "altitudeM": 247.74,
                "defaultWarnValueCm": 550.0,
                "defaultAlarmValueCm": 630.0,
                "data": [
                    {
                        "type": "height in cm",
                        "value": 358.0,
                        "requestDate": "19.04.2022T14:59:51+0200",
                        "sourceDate": "19.04.2022T14:45:00+0200"
                    }
                ],
                "trend": 10,
                "situation": 10,
                "visibility": "PUBLIC",
                "stationType": "surfacewater"
            }
        ]
    }
}
\end{minted}
\caption{Example response of the list endpoint}
\label{listing:response-list-endpoint}
\end{listing}

To retrieve historical data, the history endpoint can be used. The request URL has the following structure: \url{https://api.pegelalarm.at/api/station/1.1/<unit>/<commonid>/history?<parameters>}. The unit can either be ``height'' or ``flow''. For this thesis only height data was used. The following parameters can be set:
\begin{itemize}
    \item \textbf{loadStartDate}: The start timestamp of the queried data.
    \item \textbf{loadEndDate}: The end timestamp of the queried data.
    \item \textbf{granularity}: The granularity of the response. 
\end{itemize}
Possible values for the granularity are: ``raw'' (for the last 3 months of data), ``hour'', ``day'', ``month'', ``year'' or ``era'' (era returns one value for a given time)
When requesting aggregated data (anything other than ``raw''), the maximum value of the timespan is used. The timestamps are in the following format: ``<DD>.<MM>.<YYYY>T<HH>:<MM>:<SS><+-UTCOFFSET>'', where the `+' is URL encoded to ``\%2B''. E.g.: ``31.03.2022T13:35:40\%2B0200''. If no parameters are provided the API returns the last few datapoints.

An example request would be: \url{https://api.pegelalarm.at/api/station/1.1/height/207068-at/history?loadStartDate=01.03.2022T13:35:40%2B0200&loadEndDate=01.03.2022T18:00:00%2B0200&granularity=hour} The result of this request is shown in \autoref{listing:water-level-data-response}.
\todo{check formatting why is there a linebreak?}

\begin{listing}
\begin{minted}[linenos]{json}
{
    "status": {
        "code": 200
    },
    "payload": {
        "history": [
            {
                "value": 360.0,
                "sourceDate": "01.03.2022T13:00:00+0100"
            },
            {
                "value": 360.0,
                "sourceDate": "01.03.2022T14:00:00+0100"
            },
            {
                "value": 359.0,
                "sourceDate": "01.03.2022T15:00:00+0100"
            },
            {
                "value": 361.0,
                "sourceDate": "01.03.2022T16:00:00+0100"
            },
            {
                "value": 362.0,
                "sourceDate": "01.03.2022T17:00:00+0100"
            }
        ]
    }
}
\end{minted}
\caption{Example response of historical water level data for one station}
\label{listing:water-level-data-response}
\end{listing}

\section{Overview of the Data}
Provide an overview oth the data.
\section{Explorative Data Analysis}
Similar to overview of the data
\section{Manually detect outliers for a subset of data}
Show cases of outliers in the data and manually classify them. (Also define a way/data structure to classify outliers for time series data)
\newline
\newline
In order to speed up the manual labeling of outliers a program was written. The program is a Plotly \todo{add reference} Dash web application which displays the water level data as a scatter chart. 
By clicking the datapoints in the chart the user is able to toggle the datapoint as an outlier or back to a regular value. 
In Listing \ref{listing:manual-outlier-selection} the source code of the Dash application is shown. 
In \ref{figure:manual-outlier-selection} you can see the Website of the Python app. Below the chart a rangeslider is located, to move the zoomed in view horizontally. 
The refresh button on the left side refreshes the graph, thus updating the color and label of previously selected outliers. Furthermore it saves the data to a parquet file.
The upper and lower limit of the y-axis also gets updated, when pressing the refresh button. 
The limits are automatically set to the lowest and highest regular value. 
This was implemented, because some datasets had outliers with a huge difference towards the regular data. 
Without the automatic scaling of the y-axis, detecting other outliers with a smaller difference was not possible.
% \begin{center}
%     \makebox[\textwidth]{\includegraphics[width=\paperwidth]{...}}
%   \end{center}
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{./pics/manual-outlier-selection.png}
    \caption{Dash Webapp to classify outliers}
    \label{figure:manual-outlier-selection}
\end{figure}

% https://tex.stackexchange.com/questions/368864/pagebreak-for-minted-in-figure
% \begin{listing}
% \begin{minted}[frame=single]{python}
% print('hw')
% \end{minted}
% \caption{Code example with simple formatting}
% \label{code:manual-outlier-selection}
% \end{listing}

\subsubsection{Manual Outlier Detection Webapp using Dash}
\inputminted[linenos]{python}{./code/manual_outlier_detection.py}
\captionof{listing}{Manual Outlier Detection Webapp using Dash\label{listing:manual-outlier-selection}}
\todo{add comments to code}

\section{Outlier Detection performance Metrics}
\subsection{Confusion Matrix}
To compare the performance of two classification methods a confusion matrix, shown in \autoref{table:confusion-matrix}, can be used to provide an overview. A confusion matrix consists of the following elements (explained on the basis of outlier detection):
\begin{itemize}
    \item \ac{TP}: actual class: outlier, predicted class: outlier
    \item \ac{FN}: actual class: outlier, predicted class: regular
    \item \ac{TN}: actual class: regular, predicted class: regular
    \item \ac{FP}: actual class: regular, predicted class: outlier
\end{itemize}
\begin{table}[ht]
    \begin{tabular}{llll}
     &  & \multicolumn{2}{l}{Predicted label} \\ \cline{3-4} 
     & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{\ac{P}} & \multicolumn{1}{l|}{\ac{N}} \\ \cline{2-4} 
    \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{Positive} & \multicolumn{1}{l|}{\cellcolor[HTML]{9AFF99}True positive} & \multicolumn{1}{l|}{\cellcolor[HTML]{FFCCC9}False negative} \\ \cline{2-4} 
    \multicolumn{1}{l|}{\multirow{-2}{*}{True/ actual label}} & \multicolumn{1}{l|}{Negative} & \multicolumn{1}{l|}{\cellcolor[HTML]{FFCCC9}False positive} & \multicolumn{1}{l|}{\cellcolor[HTML]{9AFF99}True negative} \\ \cline{2-4} 
\end{tabular}
\caption{Example of binary confusion matrix}
\label{table:confusion-matrix}
\end{table}
\todo{add reference? \url{https://en.wikipedia.org/w/index.php?title=Confusion_matrix&oldid=1058352752}}
The green cells represent the correctly classified values (\ac{TP} \& \ac{TN}) and the red cells represent the incorrectly classified values (\ac{FN} \& \ac{FP}). The confusion matrix is the basis of many more performance metrics.
\subsection{Accuracy}
To make the comparison easier, the confusion matrix can be aggregated into one single value as a metric. There are numerous ways how this single metric can be calculated. One of the Most common and basic methods is the Accuracy:
\begin{equation*}
    Accuracy = \frac{Number\,of\,correct\,predictions}{Total\,number\,of\,predictions} = \frac{TP + TN}{TP + FN + TP + FP} = \frac{TP + TN}{P + N}
\end{equation*}
\todo{add caption to equations?}
The accuracy provides information what percentage of values are correctly classified. However if the classes are not equally distributed, the accuracy is a bad metric. This is especially true for heavily imbalanced classes. For example if the dataset has $1\,000\,000$ positive values and $10$ negative, when optimizing for a high accuracy it can happen that the classifier predicts every value as positive and still achieves a very high accuracy:
$$
Accuracy = \frac{1\,000\,000 + 0}{1\,000\,000 + 10} = 0.99999
$$
Thus for measuring the performance of outliers, the accuracy is not a suitable metric.

\subsection{Precision}
\begin{equation*}
    Precision = \frac{TP}{TP + FP}
\end{equation*}

\subsection{Recall}
\begin{equation*}
    Recall = \frac{TP}{TP + FN}
\end{equation*}

\subsection{F-score}
\begin{equation*}
    F-score = \frac{2 \cdot Precision \cdot Recall}{Precision + Recall}
\end{equation*}
\cite{sasakiTruthFmeasure, chinchorMUC4EvaluationMetrics1992}

This \cite{tahaMetricsEvaluating3D2015} references \cite{chinchorMUC4EvaluationMetrics1992}.\todo{chech whether this can also be included}
% Why accuracy is bad in our case. Precision, Recall, F1 score

% Define a way to compare different outlier detection models / define performance metrics. E.g. number correct outliers, average confidence for the correct outliers, number of missed outliers,....

\section{Implement different Outlier Detection Approaches}
Develop different outlier detection methods in Python and calculate performance metrics for each
\section{Compare different Outlier Detection Approaches}
Compare detection methods from the previous section.

