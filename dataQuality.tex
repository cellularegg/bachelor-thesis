\chapter{Data Quality}\label{chapter:data-quality}

\section{Features of Data Quality}\label{section:data-quality-features}
This section will provide a few example key features of data quality. 
\subsubsection{Completeness}
Data completeness describes the wholeness of data. If there are certain aspects of data missing the data is not complete. For example if each datapoint of a sensor includes the date, time and production speed, the data is not complete, if one of those features is missing or not entire, this datapoint is not complete. \cite{caiChallengesDataQuality2015, songIoTDataQuality2020}
\subsubsection{Accuracy}
The accuracy of data describes the exactness. Example for possible data which decrease the accuracy are outliers or time shifts. Usually the accuracy of data is harder to measure than the completeness, consistency, structure or documentation. Due to the heterogeneity of sensor data (regarding numerical values like production speed or temperature, not categorical values like on/off) for each datapoint it is difficult to detect which values are genuine and which are sensor errors and therefore outliers. \cite{caiChallengesDataQuality2015}
\hl{Mention this briefly because float cannot be 100\% exact, }
% https://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html
\change{Man kann hier auch den Rechenfehler von Operationen in Computern anf체hren. Bei bestimmten Operationen wird darauf im Code R체cksicht genommen.}

\subsubsection{Consistency}
One example for consistency would be, if the data interval is equal. For example there should be a datapoint every ten seconds. As soon as two datapoints are more than ten seconds apart from each other the data is not consistent anymore. \cite{caiChallengesDataQuality2015}
\subsubsection{Structure \& Documentation}
If the structure of the data is not homogeneous, it is very difficult to analyze in an automated way. As a result the data either needs to be structured from the beginning or a process needs to be fabricated to structure the data automatically. Furthermore documentation is required in order to structure and preprocess data. Documentation of data might include data format (\ac{CSV}, parquet \cite{ApacheParquet2021}, \ac{JSON}), date format (e.g. ISO 8601 with UTC offset), valid value spans (e.g. temperature is only valid if it is between 100 and 400 $^{\circ}$C)
\cite{caiChallengesDataQuality2015}

\section{Improving Data Quality}\label{section:imrpoving-data-quality}
This section will describe methods to improve data quality, based on the features elaborated in \autoref{section:data-quality-features}.
\subsubsection{Completeness}
The most common methods to increase data completeness are statistical and deep learning based approaches. The goal of these methods are to fill in the missing values of a dataset. An example for a statistical method is DynaMMo\cite{liDynaMMoMiningSummarization2009}. For \acp{ANN} \ac{LSTM} or \ac{GRU} can be used to predict missing data. \cite{songIoTDataQuality2020}
\subsubsection{Accuracy}
One approach to increase the accuracy of data is to define constraints for each value. E.g. When a machine cannot produce more than ten pieces per second, because it is physically not possible, the value could be limited to less or equal than ten. However limiting the values to a specific range might hide the fact that the machine  has an error and is producing faulty products at a rate of 15 pieces per second. This is one of the reasons why more sophisticated outlier detection methods are used. \cite{songIoTDataQuality2020}

\subsubsection{Consistency}
To facilitate consistent data, statistical smoothing or forecasting methods can be used. Examples methods are: \ac{ARIMA} or \ac{GP}. \acp{ANN} can also be used to unify the time series interval between datapoints. \cite{songIoTDataQuality2020}

\subsubsection{Structure \& Documentation}
The process of structuring heterogeneous and messy data is called data wrangling. In order to unify the structure of the data at least some documentation is required. Therefore the documentation of the data is fundamental in order to analyse or further process it. 

\section{Data Cleaning \& Cleansing Approaches}\label{section:data-cleaning-cleansing-approaches}
There are two main methods when it comes to data cleaning or data cleansing. Ignoring faulty data or replacing it with a representative value. This paper will use the term data cleaning to describe the process of ignoring or deleting incorrect data and the term data cleansing to portray the process of replacing invalid data with representative values. Faulty, incorrect, invalid or wrong data is data which is inaccurate, incomplete or inconsistent.\\
Example sensor data: (Valid values for \verb|production_speed| range from $0.00$ to $2.00$ meter(s) per minute)
\begin{table}[ht]
\begin{tabular}{|l|l|l|l|}
\hline
\verb|ID| & \verb|timestamp|        & \verb|production_speed| (meter/minute) & \verb|machine_running| \\ \hline
$0$       & 2021-12-01T12:00:00.000 & $1.56$                                 & True                   \\ \hline
$1$       & 2021-12-01T12:01:00.000 & $1.58$                                 & True                   \\ \hline
$2$       & 2021-12-01T12:02:00.000 & $3.50$                                 & True                   \\ \hline
$3$       & 2021-12-01T12:03:00.000 & $1.50$                                 & False                  \\ \hline
$4$       & 2021-12-01T12:04:00.000 & $1.50$                                 & True                   \\ \hline
$5$       & 2021-12-01T12:05:00.000 & $1.49$                                 & True                   \\ \hline
\end{tabular}
\caption{Example of IoT sensor data}
\label{table:example-iot-data}
\end{table}
\section{Data Cleaning}
As already mentioned the approach for data cleaning is to ignore or delete faulty data. Depending on the use case either the entire datapoint needs to be ignored or just one value. The process of data cleaning will be shown with the example data pictured in Table \ref{table:example-iot-data}.
The first incorrect datapoint has the \verb|ID| $2$. This row is incorrect, because the \verb|production_speed| exceeds the maximum value of $2.00$. Depending on the use case (e.g. summary of how long the machine has been running) it can make sense to just ignore the row \verb|production_speed| and keep the value for \verb|machine_running|. The second appearance of a faulty datapoint has the \verb|ID| $2$. This datapoint is incorrect since \verb|machine_running| is False but the value of \verb|production_speed| is not $0.00$. In this case it does not make sense to keep either of those values for further analysis, because it is impossible to determine which of the two columns are incorrect. A possible result after the data cleaning is shown in Table \ref{table:example-iot-data-after-cleaning}
\begin{table}[ht]
\begin{tabular}{|l|l|l|l|}
\hline
\verb|ID| & \verb|timestamp|        & \verb|production_speed| (meter/minute) & \verb|machine_running| \\ \hline
$0$       & 2021-12-01T12:00:00.000 & $1.56$                                 & True                   \\ \hline
$1$       & 2021-12-01T12:01:00.000 & $1.58$                                 & True                   \\ \hline
$2$       & 2021-12-01T12:02:00.000 &                                        & True                   \\ \hline
$3$       & 2021-12-01T12:03:00.000 &                                        &                        \\ \hline
$4$       & 2021-12-01T12:04:00.000 & $1.50$                                 & True                   \\ \hline
$5$       & 2021-12-01T12:05:00.000 & $1.49$                                 & True                   \\ \hline
\end{tabular}
\caption{Example of IoT sensor data after cleaning}
\label{table:example-iot-data-after-cleaning}
\end{table}

\section{Data Cleansing}
Data cleansing pursues a different approach. Incorrect data is not ignored, but substituted by a representative value. This can only be done, when it is sure, that no future analyisis steps depend on unaltered data. Since altering some datapoints is likely to change \acp{KPI}, which are calculated from the raw data.
% \change{Die Methode ver채ndert damit die Messung, d.h. bestimmte statistische Operationen lassen sich dann nicht mehr "sauber" durchf체hren.}
For example for the datapoint with the \verb|ID| $2$ there are several strategies that could be followed. For example the outlier value $3.50$ could be replaced with the upper limit of the valid range, in this example $2.00$, the value could also be replaced with the last valid value, in this example $1.58$, or the value could be replaced with the average of the last $n$ Values, for example with $\frac{1.56+1.58}{2} = 1.57$. For the datapoint with the \verb|ID| $3$ there are also different approaches. Either the machine was indeed not running then it would make sense, to set the \verb|production_speed| to $0.0$, if short downtimes for this machine are very unlikely then the \verb|machine_running| value could be set to True. A possible result after the data cleansing is shown in Table \ref{table:example-iot-data-after-cleansing} \cite{maleticDataCleansingIntegrity2000}
\begin{table}[ht]
\begin{tabular}{|l|l|l|l|}
\hline
\verb|ID| & \verb|timestamp|        & \verb|production_speed| (meter/minute) & \verb|machine_running| \\ \hline
$0$       & 2021-12-01T12:00:00.000 & $1.56$                                 & True                   \\ \hline
$1$       & 2021-12-01T12:01:00.000 & $1.58$                                 & True                   \\ \hline
$2$       & 2021-12-01T12:02:00.000 & $2.00$                                 & True                   \\ \hline
$3$       & 2021-12-01T12:03:00.000 & $1.50$                                 & True                   \\ \hline
$4$       & 2021-12-01T12:04:00.000 & $1.50$                                 & True                   \\ \hline
$5$       & 2021-12-01T12:05:00.000 & $1.49$                                 & True                   \\ \hline
\end{tabular}
\caption{Example of IoT sensor data after cleansing}
\label{table:example-iot-data-after-cleansing}
\end{table}