
@misc{ApacheParquet2021,
  title = {Apache {{Parquet}}},
  year = {2021},
  month = dec,
  url = {https://parquet.apache.org/}
}

@book{Asdf,
  title = {Asdf}
}

@article{baeOutlierDetectionSmoothing2019,
  title = {Outlier {{Detection}} and {{Smoothing Process}} for {{Water Level Data Measured}} by {{Ultrasonic Sensor}} in {{Stream Flows}}},
  author = {Bae, Inhyeok and Ji, Un},
  year = {2019},
  month = may,
  journal = {Water},
  volume = {11},
  number = {5},
  pages = {951},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2073-4441},
  doi = {10.3390/w11050951},
  url = {https://www.mdpi.com/2073-4441/11/5/951},
  urldate = {2022-04-15},
  abstract = {Water level data sets acquired by ultrasonic sensors in stream-scale channels exhibit relatively large numbers of outliers that are off the measurement range between the ultrasonic sensor and water surface, as well as data dispersion of approximately 2 cm due to random errors such as water waves. Therefore, this study develops a data processing algorithm for outlier removal and smoothing for water level data measured by ultrasonic sensors to consider these characteristics. The outlier removal process includes an initial cutoff process to remove outliers out of the measurement range and an outlier detection process using modified Z-scores based on the median absolute deviation (MAD) of a robust estimator. In addition, an exponentially weighted moving average (EWMA) method is applied to smooth the processed data. Sensitivity analyses are performed for factors that are subjectively set by the user, including the window size for the MAD outlier detection stage, the rejection criterion for the modified Z-score outlier removal stage, and the smoothing constant for the EWMA smoothing stage, based on four different water level data sets acquired by ultrasonic sensors in stream-scale experiments.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {data smoothing,exponentially weighted moving average,median absolute deviation,modified Z-scores,outlier detection,ultrasonic sensor,water level monitoring},
  file = {/home/zeleodav/Zotero/storage/QQ8UV82H/Bae and Ji - 2019 - Outlier Detection and Smoothing Process for Water .pdf;/home/zeleodav/Zotero/storage/7XMHISR3/951.html}
}

@article{basuAutomaticOutlierDetection2007,
  title = {Automatic Outlier Detection for Time Series: {{An}} Application to Sensor Data},
  shorttitle = {Automatic Outlier Detection for Time Series},
  author = {Basu, Sabyasachi and Meckesheimer, Martin},
  year = {2007},
  month = feb,
  journal = {Knowledge and Information Systems},
  volume = {11},
  number = {2},
  pages = {137--154},
  issn = {0219-3116},
  doi = {10.1007/s10115-006-0026-6},
  abstract = {In this article we consider the problem of detecting unusual values or outliers from time series data where the process by which the data are created is difficult to model. The main consideration is the fact that data closer in time are more correlated to each other than those farther apart. We propose two variations of a method that uses the median from a neighborhood of a data point and a threshold value to compare the difference between the median and the observed data value. Both variations of the method are fast and can be used for data streams that occur in quick succession such as sensor data on an airplane.},
  langid = {english},
  file = {/home/zeleodav/Zotero/storage/H4NV2ITU/Basu and Meckesheimer - 2007 - Automatic outlier detection for time series An ap.pdf}
}

@article{blazquez-garciaReviewOutlierAnomaly2020,
  title = {A Review on {{Outlier}}/{{Anomaly}} Detection in Time Series Data},
  author = {{Bl{\'a}zquez-Garc{\'i}a}, Ane and Conde, Angel and Mori, Usue and Lozano, Jose A.},
  year = {2020},
  month = feb,
  journal = {arXiv:2002.04236 [cs, stat]},
  eprint = {2002.04236},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Recent advances in technology have brought major breakthroughs in data collection, enabling a large amount of data to be gathered over time and thus generating time series. Mining this data has become an important task for researchers and practitioners in the past few years, including the detection of outliers or anomalies that may represent errors or events of interest. This review aims to provide a structured and comprehensive state-of-the-art on outlier detection techniques in the context of time series. To this end, a taxonomy is presented based on the main aspects that characterize an outlier detection technique.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{caiChallengesDataQuality2015,
  title = {The {{Challenges}} of {{Data Quality}} and {{Data Quality Assessment}} in the {{Big Data Era}}},
  author = {Cai, Li and Zhu, Yangyong},
  year = {2015},
  month = may,
  journal = {Data Science Journal},
  volume = {14},
  number = {0},
  pages = {2},
  publisher = {{Ubiquity Press}},
  issn = {1683-1470},
  doi = {10.5334/dsj-2015-002},
  abstract = {High-quality data are the precondition for analyzing and using big data and for guaranteeing the value of the data. Currently, comprehensive analysis and research of quality standards and quality assessment methods for big data are lacking. First, this paper summarizes reviews of data quality research. Second, this paper analyzes the data characteristics of the big data environment, presents quality challenges faced by big data, and formulates a hierarchical data quality framework from the perspective of data users. This framework consists of big data quality dimensions, quality characteristics, and quality indexes. Finally, on the basis of this framework, this paper constructs a dynamic assessment process for data quality. This process has good expansibility and adaptability and can meet the needs of big data quality assessment. The research results enrich the theoretical scope of big data and lay a solid foundation for the future by establishing an assessment model and studying evaluation algorithms.},
  copyright = {Authors who publish with this journal agree to the following terms: Authors retain copyright and grant the journal right of first publication with the work simultaneously licensed under a Creative Commons Attribution License that allows others to share the work with an acknowledgement of the work's authorship and initial publication in this journal. Authors are able to enter into separate, additional contractual arrangements for the non-exclusive distribution of the journal's published version of the work (e.g., post it to an institutional repository or publish it in a book), with an acknowledgement of its initial publication in this journal. Authors are permitted and encouraged to post their work online (e.g., in institutional repositories or on their website) prior to and during the submission process, as it can lead to productive exchanges, as well as earlier and greater citation of published work (See The Effect of Open Access ). All third-party images reproduced on this journal are shared under Educational Fair Use. For more information on Educational Fair Use , please see this useful checklist prepared by Columbia University Libraries . All copyright of third-party content posted here for research purposes belongs to its original owners. Unless otherwise stated all references to characters and comic art presented on this journal are \textcopyright, \textregistered{} or \texttrademark{} of their respective owners. No challenge to any owner's rights is intended or should be inferred.},
  langid = {english},
  file = {/home/zeleodav/Zotero/storage/ZUKPYT27/Cai and Zhu - 2015 - The Challenges of Data Quality and Data Quality As.pdf}
}

@article{chandolaAnomalyDetectionSurvey2009,
  title = {Anomaly Detection: {{A}} Survey},
  shorttitle = {Anomaly Detection},
  author = {Chandola, Varun and Banerjee, Arindam and Kumar, Vipin},
  year = {2009},
  month = jul,
  journal = {ACM Computing Surveys},
  volume = {41},
  number = {3},
  pages = {15:1--15:58},
  issn = {0360-0300},
  doi = {10.1145/1541880.1541882},
  abstract = {Anomaly detection is an important problem that has been researched within diverse research areas and application domains. Many anomaly detection techniques have been specifically developed for certain application domains, while others are more generic. This survey tries to provide a structured and comprehensive overview of the research on anomaly detection. We have grouped existing techniques into different categories based on the underlying approach adopted by each technique. For each category we have identified key assumptions, which are used by the techniques to differentiate between normal and anomalous behavior. When applying a given technique to a particular domain, these assumptions can be used as guidelines to assess the effectiveness of the technique in that domain. For each category, we provide a basic anomaly detection technique, and then show how the different existing techniques in that category are variants of the basic technique. This template provides an easier and more succinct understanding of the techniques belonging to each category. Further, for each category, we identify the advantages and disadvantages of the techniques in that category. We also provide a discussion on the computational complexity of the techniques since it is an important issue in real application domains. We hope that this survey will provide a better understanding of the different directions in which research has been done on this topic, and how techniques developed in one area can be applied in domains for which they were not intended to begin with.},
  keywords = {Anomaly detection,outlier detection}
}

@article{cookAnomalyDetectionIoT2020,
  title = {Anomaly {{Detection}} for {{IoT Time-Series Data}}: {{A Survey}}},
  shorttitle = {Anomaly {{Detection}} for {{IoT Time-Series Data}}},
  author = {Cook, Andrew A. and M{\i}s{\i}rl{\i}, G{\"o}ksel and Fan, Zhong},
  year = {2020},
  month = jul,
  journal = {IEEE Internet of Things Journal},
  volume = {7},
  number = {7},
  pages = {6481--6494},
  issn = {2327-4662},
  doi = {10.1109/JIOT.2019.2958185},
  abstract = {Anomaly detection is a problem with applications for a wide variety of domains; it involves the identification of novel or unexpected observations or sequences within the data being captured. The majority of current anomaly detection methods are highly specific to the individual use case, requiring expert knowledge of the method as well as the situation to which it is being applied. The Internet of Things (IoT) as a rapidly expanding field offers many opportunities for this type of data analysis to be implemented, however, due to the nature of the IoT, this may be difficult. This review provides a background on the challenges which may be encountered when applying anomaly detection techniques to IoT data, with examples of applications for the IoT anomaly detection taken from the literature. We discuss a range of approaches that have been developed across a variety of domains, not limited to IoT due to the relative novelty of this application. Finally, we summarize the current challenges being faced in the anomaly detection domain with a view to identifying potential research opportunities for the future.},
  keywords = {Anomaly detection,data analysis,Data analysis,Internet of Things,Internet of Things (IoT),Monitoring,Performance evaluation,Sensors,survey,Urban areas},
  file = {/home/zeleodav/Zotero/storage/JDCTNGVI/Cook et al. - 2020 - Anomaly Detection for IoT Time-Series Data A Surv.pdf}
}

@article{giannoniAnomalyDetectionModels2018,
  title = {Anomaly {{Detection Models}} for {{IoT Time Series Data}}},
  author = {Giannoni, Federico and Mancini, Marco and Marinelli, Federico},
  year = {2018},
  month = nov,
  journal = {arXiv:1812.00890 [cs, eess]},
  eprint = {1812.00890},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {Insitu sensors and Wireless Sensor Networks (WSNs) have become more and more popular in the last decade, due to their potential to be used in various applications of many different fields. As of today, WSNs are pretty much used by any monitoring system: from those that are health care related, to those that are used for environmental forecasting or surveillance purposes. All applications that make use of insitu sensors, strongly rely on their correct operation, which however, is quite difficult to guarantee. These sensors in fact, are typically cheap and prone to malfunction. Additionally, for many tasks (e.g. environmental forecasting), sensors are also deployed under potentially harsh weather condition, making their breakage even more likely. The high probability of erroneous readings or data corruption during transmission, brings up the problem of ensuring quality of the data collected by sensors. Since WSNs have to operate continuously and therefore generate very large volumes of data every day, the quality control process has to be automated, scalable and fast enough to be applicable to streaming data. The most common approach to ensure the quality of sensors data, consists in automated detection of erroneous readings or anomalous behaviours of sensors. In the literature, this strategy is known as anomaly detection and can be pursued in many different ways.},
  archiveprefix = {arXiv},
  keywords = {68-00,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Signal Processing}
}

@misc{GitHub,
  title = {{{GitHub}}},
  shorttitle = {{{GitHub}}},
  journal = {GitHub},
  url = {https://github.com/},
  urldate = {2022-04-19},
  abstract = {GitHub is where over 73 million developers shape the future of software, together. Contribute to the open source community, manage your Git repositories, review code like a pro, track bugs and feat...},
  langid = {english},
  file = {/home/zeleodav/Zotero/storage/PZSB8N3A/github.com.html}
}

@misc{HmacKeyedHashingMessage,
  title = {Hmac \textemdash{} {{Keyed-Hashing}} for {{Message Authentication}} \textemdash{} {{Python}} 3.9.12 Documentation},
  url = {https://docs.python.org/3.9/library/hmac.html},
  urldate = {2022-04-19}
}

@article{leysDetectingOutliersNot2013,
  title = {Detecting Outliers: {{Do}} Not Use Standard Deviation around the Mean, Use Absolute Deviation around the Median},
  shorttitle = {Detecting Outliers},
  author = {Leys, Christophe and Ley, Christophe and Klein, Olivier and Bernard, Philippe and Licata, Laurent},
  year = {2013},
  month = jul,
  journal = {Journal of Experimental Social Psychology},
  volume = {49},
  number = {4},
  pages = {764--766},
  issn = {0022-1031},
  doi = {10.1016/j.jesp.2013.03.013},
  abstract = {A survey revealed that researchers still seem to encounter difficulties to cope with outliers. Detecting outliers by determining an interval spanning over the mean plus/minus three standard deviations remains a common practice. However, since both the mean and the standard deviation are particularly sensitive to outliers, this method is problematic. We highlight the disadvantages of this method and present the median absolute deviation, an alternative and more robust measure of dispersion that is easy to implement. We also explain the procedures for calculating this indicator in SPSS and R software.},
  langid = {english},
  keywords = {MAD,Median absolute deviation,Outlier}
}

@inproceedings{liDynaMMoMiningSummarization2009,
  title = {{{DynaMMo}}: {{Mining}} and Summarization of Coevolving Sequences with Missing Values},
  shorttitle = {{{DynaMMo}}},
  booktitle = {Proceedings of the 15th {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining},
  author = {Li, Lei and McCann, James and Pollard, Nancy S. and Faloutsos, Christos},
  year = {2009},
  month = jun,
  series = {{{KDD}} '09},
  pages = {507--516},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1557019.1557078},
  abstract = {Given multiple time sequences with missing values, we propose DynaMMo which summarizes, compresses, and finds latent variables. The idea is to discover hidden variables and learn their dynamics, making our algorithm able to function even when there are missing values. We performed experiments on both real and synthetic datasets spanning several megabytes, including motion capture sequences and chlorine levels in drinking water. We show that our proposed DynaMMo method (a) can successfully learn the latent variables and their evolution; (b) can provide high compression for little loss of reconstruction accuracy; (c) can extract compact but powerful features for segmentation, interpretation, and forecasting; (d) has complexity linear on the duration of sequences.},
  isbn = {978-1-60558-495-9},
  keywords = {bayesian network,expectation maximization (em),missing value,time series}
}

@article{maleticDataCleansingIntegrity2000,
  title = {Data {{Cleansing}}: {{Beyond Integrity Analysis}}},
  author = {Maletic, Jonathan I and Marcus, Andrian},
  year = {2000},
  pages = {10},
  abstract = {The paper analyzes the problem of data cleansing and automatically identifying potential errors in data sets. An overview of the diminutive amount of existing literature concerning data cleansing is given. Methods for error detection that go beyond integrity analysis are reviewed and presented. The applicable methods include: statistical outlier detection, pattern matching, clustering, and data mining techniques. Some brief results supporting the use of such methods are given. The future research directions necessary to address the data cleansing problem are discussed.},
  langid = {english}
}

@inproceedings{mehrangOutlierDetectionWeight2015,
  title = {Outlier Detection in Weight Time Series of Connected Scales},
  booktitle = {2015 {{IEEE International Conference}} on {{Bioinformatics}} and {{Biomedicine}} ({{BIBM}})},
  author = {Mehrang, Saeed and Helander, Elina and Pavel, Misha and Chieh, Angela and Korhonen, Ilkka},
  year = {2015},
  month = nov,
  pages = {1489--1496},
  doi = {10.1109/BIBM.2015.7359896},
  abstract = {In principle, connected sensors allow effortless long-term self-monitoring of health and wellness that can help maintain health and quality of life. However, data collected in the ``wild'' may be noisy and contain outliers, e.g., due to uncontrolled sources or data from different persons using the same device. The removal of the ``outliers'' is therefore critical for accurate interpretation of the data. In this paper we study the detection and elimination of outliers in self-weighing time series data obtained from connected weight scales. We examined three techniques: (1) a method based on autoregressive integrated moving average (ARIMA) time series modelling, (2) median absolute deviation (MAD) scale estimate, and (3) a method based on Rosner statistics. We applied these methods to both a data set with real outliers and a clean data set corrupted with simulated outliers. The results suggest that the simple MAD algorithm and ARIMA performed well with both test sets while the Rosner statistics was significantly less effective. In addition, the ARIMA approach appeared to be significantly less sensitive to long periods of missing data than MAD and Rosner statistics.},
  keywords = {ARIMA modeling,Electrostatic discharges,Lead,MAD scale estimate,Monitoring,outlier detection,Rosner statistics,Weight measurement,weight time series analysis}
}

@article{ridzuanReviewDataCleansing2019,
  title = {A {{Review}} on {{Data Cleansing Methods}} for {{Big Data}}},
  author = {Ridzuan, Fakhitah and Wan Zainon, Wan Mohd Nazmee},
  year = {2019},
  month = jan,
  journal = {Procedia Computer Science},
  series = {The {{Fifth Information Systems International Conference}}, 23-24 {{July}} 2019, {{Surabaya}}, {{Indonesia}}},
  volume = {161},
  pages = {731--738},
  issn = {1877-0509},
  doi = {10.1016/j.procs.2019.11.177},
  abstract = {Massive amounts of data are available for the organization which will influence their business decision. Data collected from the various resources are dirty and this will affect the accuracy of prediction result. Data cleansing offers a better data quality which will be a great help for the organization to make sure their data is ready for the analyzing phase. However, the amount of data collected by the organizations has been increasing every year, which is making most of the existing methods no longer suitable for big data. Data cleansing process mainly consists of identifying the errors, detecting the errors and corrects them. Despite the data need to be analyzed quickly, the data cleansing process is complex and time-consuming in order to make sure the cleansed data have a better quality of data. The importance of domain expert in data cleansing process is undeniable as verification and validation are the main concerns on the cleansed data. This paper reviews the data cleansing process, the challenge of data cleansing for big data and the available data cleansing methods.},
  langid = {english},
  keywords = {big data,data cleansing,data quality}
}

@incollection{songIoTDataQuality2020,
  title = {{{IoT Data Quality}}},
  booktitle = {Proceedings of the 29th {{ACM International Conference}} on {{Information}} \& {{Knowledge Management}}},
  author = {Song, Shaoxu and Zhang, Aoqian},
  year = {2020},
  month = oct,
  pages = {3517--3518},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  abstract = {Data quality issues have been widely recognized in IoT data, and prevent the downstream applications. In this tutorial, we review the state-of-the-art techniques for IoT data quality management. In particular, we discuss how the dedicated approaches improve various data quality dimensions, including validity, completeness and consistency. Among others, we further highlight the recent advances by deep learning techniques for IoT data quality. Finally, we indicate the open problems in IoT data quality management, such as benchmark or interpretation of data quality issues.},
  isbn = {978-1-4503-6859-9},
  keywords = {data curation,internet of things}
}

@misc{strassmayrPegelalarmAPIWrapper2022,
  title = {Pegelalarm {{API}} Wrapper},
  author = {Strassmayr, Johannes},
  year = {2022},
  month = apr,
  url = {https://github.com/SOBOS-GmbH/pegelalarm_public_pas_doc},
  urldate = {2022-04-19},
  abstract = {This project holds the public documentation for the PegelAlarm Server API},
  howpublished = {SOBOS-GmbH}
}

@article{tehSensorDataQuality2020,
  title = {Sensor Data Quality: {{A}} Systematic Review},
  shorttitle = {Sensor Data Quality},
  author = {Teh, Hui Yie and {Kempa-Liehr}, Andreas W. and Wang, Kevin I-Kai},
  year = {2020},
  month = feb,
  journal = {Journal of Big Data},
  volume = {7},
  number = {1},
  pages = {11},
  issn = {2196-1115},
  doi = {10.1186/s40537-020-0285-1},
  abstract = {Sensor data quality plays a vital role in Internet of Things (IoT) applications as they are rendered useless if the data quality is bad. This systematic review aims to provide an introduction and guide for researchers who are interested in quality-related issues of physical sensor data. The process and results of the systematic review are presented which aims to answer the following research questions: what are the different types of physical sensor data errors, how to quantify or detect those errors, how to correct them and what domains are the solutions in. Out of 6970 literatures obtained from three databases (ACM Digital Library, IEEE Xplore and ScienceDirect) using the search string refined via topic modelling, 57 publications were selected and examined. Results show that the different types of sensor data errors addressed by those papers are mostly missing data and faults e.g. outliers, bias and drift. The most common solutions for error detection are based on principal component analysis (PCA) and artificial neural network (ANN) which accounts for about 40\% of all error detection papers found in the study. Similarly, for fault correction, PCA and ANN are among the most common, along with Bayesian Networks. Missing values on the other hand, are mostly imputed using Association Rule Mining. Other techniques include hybrid solutions that combine several data science methods to detect and correct the errors. Through this systematic review, it is found that the methods proposed to solve physical sensor data errors cannot be directly compared due to the non-uniform evaluation process and the high use of non-publicly available datasets. Bayesian data analysis done on the 57 selected publications also suggests that publications using publicly available datasets for method evaluation have higher citation rates.},
  keywords = {Datasets,Sensor data error correction,Sensor data error detection,Sensor data quality,Systematic review},
  file = {/home/zeleodav/Zotero/storage/GVDQVK6L/Teh et al. - 2020 - Sensor data quality A systematic review.pdf}
}

@article{weissMetricVisualizationCompleteness2021,
  title = {A {{Metric}} and {{Visualization}} of {{Completeness}} in {{Multi-Dimensional Data Sets}} of {{Sensor}} and {{Actuator Data Applied}} to a {{Condition Monitoring Use Case}}},
  author = {Wei{\ss}, Iris and {Vogel-Heuser}, Birgit},
  year = {2021},
  month = jan,
  journal = {Applied Sciences},
  volume = {11},
  number = {11},
  pages = {5022},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  doi = {10.3390/app11115022},
  abstract = {The so-called `Industrie 4.0' provides high potential for data-driven methods in automated production systems. However, sensor and actuator data gathered during normal operation of the system is often limited to a narrow range of single, specific operating points. This limitation also restricts the significance of condition-based maintenance models, which are trained to the narrow data. In order to reveal the structure of such multi-dimensional data sets and detect deficiencies, this paper derives a data quality metric and visualization. The metric observes the feature space and evaluates the completeness of data. In the best case, the observations utilize the whole feature space, meaning all different combinations of the variables are present in the data. Low metric values indicate missing combinations, reducing the representativeness of the data. In this way, appropriate countermeasures can be taken if relevant data is missing. For evaluation, a data set of an industrial test bed for condition monitoring of control valves is used. It is shown that the state-of-the-art metrics and visualizations cannot detect deficiencies of completeness in multi-dimensional data sets. In contrast, the proposed heat map enables the expert to locate limitations in multi-dimensional data sets.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {automated production systems,completeness,condition monitoring,control valves,data quality assessment,data quality metric,industrie 4.0,sensor and actuator data},
  file = {/home/zeleodav/Zotero/storage/5F8AU5W4/Weiß and Vogel-Heuser - 2021 - A Metric and Visualization of Completeness in Mult.pdf}
}

@article{yuRecursivePrincipalComponent2017,
  title = {Recursive {{Principal Component Analysis-Based Data Outlier Detection}} and {{Sensor Data Aggregation}} in {{IoT Systems}}},
  author = {Yu, Tianqi and Wang, Xianbin and Shami, Abdallah},
  year = {2017},
  month = dec,
  journal = {IEEE Internet of Things Journal},
  volume = {4},
  number = {6},
  pages = {2207--2216},
  issn = {2327-4662},
  doi = {10.1109/JIOT.2017.2756025},
  abstract = {Internet of Things (IoT) is emerging as the underlying technology of our connected society, which enables many advanced applications. In IoT-enabled applications, information of application surroundings is gathered by networked sensors, especially wireless sensors due to their advantage of infrastructure-free deployment. However, the pervasive deployment of wireless sensor nodes generate massive amount of sensor data, and data outliers are frequently incurred due to the dynamic nature of wireless channels. As operation of IoT systems relies on sensor data, data redundancy and data outliers could significantly reduce the effectiveness of IoT applications or even mislead systems into unsafe conditions. In this paper, a cluster-based data analysis framework is proposed using recursive principal component analysis (R-PCA), which can aggregate the redundant data and detect the outliers in the meantime. More specifically, at a cluster head, spatially correlated sensor data collected from cluster members are aggregated by extracting the principal components (PCs), and potential data outliers are determined by the abnormal squared prediction error score, which is defined as the square of residual value after extraction of PCs. With R-PCA, the parameters of PCA model can be recursively updated to adapt to the changes in IoT systems. Cluster-based data analysis framework also releases the computational and processing burdens on sensor nodes. Practical databases-based simulations have confirmed that the proposed framework efficiently aggregates the correlated sensor data with high recovery accuracy. The data outlier detection accuracy is also improved by the proposed method compared to other existing algorithms.},
  keywords = {Algorithm design and analysis,Anomaly detection,Correlation,Data aggregation,Data analysis,Internet of Things (IoT),outlier detection,Principal component analysis,recursive principal component analysis (R-PCA),Wireless sensor networks}
}

@misc{zelenayBachelorThesisCode2022,
  title = {Bachelor {{Thesis Code}}},
  author = {Zelenay, David},
  year = {2022},
  month = apr,
  url = {https://github.com/cellularegg/bachelor-thesis-code},
  urldate = {2022-04-19},
  abstract = {Code for the bachelor thesis}
}

@article{zhangAdaptiveOutlierDetection2019,
  title = {An {{Adaptive Outlier Detection}} and {{Processing Approach Towards Time Series Sensor Data}}},
  author = {Zhang, Minghu and Li, Xin and Wang, Lili},
  year = {2019},
  journal = {IEEE access : practical innovations, open solutions},
  volume = {7},
  pages = {175192--175212},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2957602},
  abstract = {The intelligent environment monitoring network, as the foundation of ecosystem research, has rapidly developed with the ever-growing Internet of Things (IoT). IoT-networked sensors deployed to monitor ecosystems generate copious sensor data characterized by nonstationarity and nonlinearity such that outlier detection remains a source of concern. Most outlier detection models involve hypothesis tests based on setting outlier threshold values. However, signal decomposition describes stationary and nonstationary relationships sensor data. Therefore, this paper proposes a three-level hybrid model based on the median filter (MF), empirical mode decomposition (EMD), classification and regression tree (CART), autoregression (AR) and exponential weighted moving average (EWMA) methods called MF-EMD-CART-AR-EWMA to detect outliers in sensor data. The first-level performance is compared to that of the Butterworth filter, FIR filter, moving average filter, wavelet filter and Wiener filter. The second-level prediction performance is compared to support vector regression (SVR), K-nearest neighbor (KNN), CART, complementary ensemble EEMD with CART and AR (EEMD-CART-AR) and ensemble CEEMD with CART and AR (CEEMD-CART-AR) methods. Finally, EWMA is compared to Cumulative Sum Control Chart (CUSUM) and Shewhart control charts. The proposed hybrid model was evaluated with a real dataset from the hydrometeorological observation network in the Heihe River Basin, yielding experimental results with better generalization ability and higher accuracy than the compared models, and providing extremely effective detection of minor outliers in predicted values. This paper provides valuable insight and a promising reference for outlier detection involving sensor data and presents a new perspective for detecting outliers.},
  keywords = {Analytical models,Anomaly detection,Autoregressive processes,Data models,Environmental monitoring,integrated model,Monitoring,outlier detection,Predictive models,sensor data,statistical analysis,Time series analysis},
  file = {/home/zeleodav/Zotero/storage/KSXX3974/Zhang et al. - 2019 - An Adaptive Outlier Detection and Processing Appro.pdf}
}


